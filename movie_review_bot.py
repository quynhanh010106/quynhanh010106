# -*- coding: utf-8 -*-
"""Movie_Review_Bot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A9gOFRpPICwTkPneHg3acE-l8Zn7uqK7
"""

!pip install openai==0.28.1



import openai
from getpass import getpass

# Prompt the user to enter the API key securely
api_key = getpass("Enter your OpenAI API key: ")

# Set the API key
openai.api_key = api_key

# Rest of your code goes here

!pip install gradio

!pip install pandas
!pip install tensorflow
!pip install tensorflow_hub
!pip install numpy

!ls /content/rating.xlsx

import pandas as pd

# Load the dataset from the uploaded file
file_path_notitle = '/content/rating-short.xlsx'
imdb_data_notitle = pd.read_excel(file_path_notitle)

file_path = '/content/rating-short - Copy.xlsx'
imdb_data = pd.read_excel(file_path)

# Print the actual column names in your DataFrame
print(imdb_data.columns)

# Display the first few rows of the dataset to understand its structure
imdb_data.head()
# Assuming the column is named 'tconst', adjust the code accordingly
movies = imdb_data[' tconst']
ratings = imdb_data['averageRating']

# Display a few titles and ratings to verify
movies.head(), ratings.head()

# Create a dictionary mapping movie titles to ratings
title_to_rating = dict(zip(imdb_data['title'], imdb_data['averageRating']))
title_to_id = dict(zip(imdb_data['title'], imdb_data[' tconst']))

!pip install requests beautifulsoup4==4.12.2

import requests
from bs4 import BeautifulSoup

def get_movie_name(imdb_id):
  url = f"https://www.imdb.com/title/{imdb_id}/"
  headers = {'User-Agent': 'Mozilla/5.0'}
# Add a user-agent header
  response = requests.get(url, headers=headers)
  soup = BeautifulSoup(response.content, 'html.parser')
  title = soup.find('h1').text.strip()
  return title
movie_names = []
for imdb_id in imdb_data[' tconst']:
  movie_names.append(get_movie_name(imdb_id))

# Print the list of movie names
print(movie_names)

import numpy as np

# Assuming 'title_to_rating' dictionary is already created as in previous response

def query_imdb(user_input):
    if user_input in title_to_rating:
        rating = title_to_rating[user_input]
        return f"The rating for {user_input} is {rating}"
    else:
        return f"{user_input} not found in the dataset."

def get_imdb_url(user_input):
  # Find the row corresponding to the movie name
  row = imdb_data.loc[imdb_data['title'] == user_input]  # Assuming 'originalTitle' column contains movie names

  if not row.empty:
    imdb_id = row[' tconst'].values[0]  # Extract the 'tconst' value
    url = f"https://www.imdb.com/title/{imdb_id}/"
    headers = {'User-Agent': 'Mozilla/5.0'}
    return url, headers
  else:
    return None, None

# Example usage
url, headers = get_imdb_url("Oppenheimer")
if url:
  print(url)
  print(headers)
else:
  print("Movie not found in the dataset.")

import openai
import gradio as gr

messages = [{"role": "system", "content": "You are a movie expert. When responding, always structure your answer into three distinct sections with headings: Summary, Analysis, and Famous Comments. Provide a concise response for each section."}]

def CustomChatGPT(user_input):
    # Query IMDb dataset
    imdb_response = query_imdb(user_input)

    imdb_url, _ = get_imdb_url(user_input)
    if imdb_url:
        imdb_response += f"\n\nIMDb URL: {imdb_url}"

    messages.append({"role": "user", "content": user_input})

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages
    )
    ChatGPT_reply = response["choices"][0]["message"]["content"]

    # Integrate IMDb response into the ChatGPT reply
    chatgpt_reply_with_imdb = f"{imdb_response}\n\n{ChatGPT_reply}"

    messages.append({"role": "assistant", "content": chatgpt_reply_with_imdb})
    return chatgpt_reply_with_imdb

demo = gr.Interface(fn=CustomChatGPT, inputs="text", outputs="text", title="Movie Review Pro")
demo.launch(share=True)

